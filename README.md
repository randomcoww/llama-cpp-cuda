### Add llama-swap and tini to llama.cpp server CUDA image

https://github.com/ggml-org/llama.cpp
