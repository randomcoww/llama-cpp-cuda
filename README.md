### Container for llama.cpp server built for specific CUDA versions

https://github.com/ggml-org/llama.cpp