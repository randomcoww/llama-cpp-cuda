### Container build for llama.cpp server + llama-swap on newer CUDA

https://github.com/ggml-org/llama.cpp
